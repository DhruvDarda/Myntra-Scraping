{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Persuasive_NLP_Seq2Seq",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdofT3K2D/CzxA0Qc+QZJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvDarda/Persuasive-Text-Generation-in-Fashion/blob/main/Persuasive_NLP_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkTSQTvbRBMW"
      },
      "source": [
        "Persuasive text often includes:\n",
        "\n",
        "repeated words,\n",
        "\n",
        "alliterative words,\n",
        "\n",
        "emotional language,\n",
        "\n",
        "a strong argument,\n",
        "\n",
        "rhetorical questions,\n",
        "\n",
        "colourful and eye-catching fonts / capitalised words,\n",
        "\n",
        "humour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbt_ZK6oRWzi"
      },
      "source": [
        "Words with high modality, that is, words that show a high degree of certainty. For example – must, ought to, shall, has to. In comparison to words such as may, might, could and would that have low modality and show less certainty\n",
        "\n",
        "Emotive, descriptive words that appeal to the emotions. For example – wonderful, horrible, cruel, amazing, frightening, perfect\n",
        "\n",
        "A formal voice that is more authoritative and has more power of persuasion\n",
        "\n",
        "Repetition of words or phrases and concepts to push your point of view\n",
        "\n",
        "Connectives that help sequence your argument. For example – Firstly, Secondly\n",
        "\n",
        "Present tense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPLjN8S_GziU"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "from torch import Tensor \n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXO1hxuxzWeG",
        "outputId": "d83bb95e-27c4-44c0-9c6e-15af97892c74"
      },
      "source": [
        "from google.colab import drive\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "%cd /content/gdrive/My Drive/'NLP PROJECT'\n",
        "writer = SummaryWriter('Transformer/seq2seq')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/My Drive/NLP PROJECT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZynxaIQ9iko",
        "outputId": "5373e000-6142-4204-a0b5-7389f19a852b"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ethnic_Wear_Data.csv   my_checkpoint.pth.tar     \u001b[0m\u001b[01;34mruns\u001b[0m/       \u001b[01;34mTransformer\u001b[0m/\n",
            "Ethnic_wear_final.csv  my_checkpointseq.pth.tar  test.csv    tut1-model.pt\n",
            "inputs_final1.txt      outputs_final.txt         test.json\n",
            "inputs_large.txt       outputs_large.txt         train.csv\n",
            "inputs.txt             outputs.txt               train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFLqd0_UTP86"
      },
      "source": [
        "inputs = open('inputs_large.txt').read().split('\\n')\n",
        "outputs = open('outputs_large.txt').read().split('\\n')\n",
        "inputs = inputs[:min(len(inputs), len(outputs))]\n",
        "outputs = outputs[:min(len(inputs), len(outputs))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om8_cN-aMfta"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OISZgLKMZ6NW"
      },
      "source": [
        "###First Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwYiCaB-dTk4",
        "outputId": "608b6dc5-6399-4fc6-9568-56d330a42803"
      },
      "source": [
        "raw_data = {'inputs':[line for line in inputs],\n",
        "            'outputs': [line for line in outputs]}\n",
        "df = pd.DataFrame(raw_data, columns = ['inputs', 'outputs'])\n",
        "print(df.head())\n",
        "\n",
        "train, test = train_test_split(df, test_size = 0.2)\n",
        "\n",
        "train.to_json('train.json', orient='records', lines = True)\n",
        "test.to_json('test.json', orient='records', lines = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              inputs                                            outputs\n",
            "0  Women Navy Blue & amp ; Rust Orange Printed St...  This fashionable kurta from anayna will take y...\n",
            "1  Green & amp ; Gold - Toned Silk Blend Embroide...  This fashionable kurta from anayna will take y...\n",
            "2  Women Green & amp ; Pink Embroidered Straight ...  Comfortable and stylish, this kurta from Vbuyz...\n",
            "3  Dusty Pink Embroidered Semi - Stitched Lehenga...  Comfortable and stylish, this kurta from Vbuyz...\n",
            "4  Women Pink Ethnic Motifs Yoke Design Regular G...  Comfortable and stylish, this kurta from Vbuyz...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TTO-DX-fV-D"
      },
      "source": [
        "#!pip install -U torchtext==0.8.0\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK-boxXuf3sM"
      },
      "source": [
        "eng = spacy.load('en')\n",
        "def tokenize_text(text):\n",
        "  return [tok.text for tok in eng.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhprntZ3nsJU"
      },
      "source": [
        "inp_fields = Field(use_vocab = True, tokenize = tokenize_text, lower = False, fix_length=100, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "out_fields = Field(use_vocab = True, tokenize = tokenize_text, lower = False, fix_length=100, init_token=\"<sos>\", eos_token=\"<eos>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-c5f6Ho1wn8"
      },
      "source": [
        "train_data, test_data = TabularDataset.splits(path='', train = 'train.json', test = 'test.json', format='json', fields = {'inputs':('text', inp_fields), 'outputs':('label', out_fields)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuTWQhyQ68Bv"
      },
      "source": [
        "inp_fields.build_vocab(train_data, max_size = df.size, min_freq = 2)\n",
        "out_fields.build_vocab(test_data, max_size = df.size, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YjaJCE3Klqr"
      },
      "source": [
        "train_itr, test_itr = BucketIterator.splits((train_data, test_data), batch_size=32, device = device, sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc_IkBYNK75X"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "class ModelParam(object):\n",
        "    def __init__(self, param_dict: dict = dict()):\n",
        "        self.input_size = param_dict.get('input_size', 0)\n",
        "        self.vocab_size = param_dict.get('vocab_size')\n",
        "        self.embedding_dim = param_dict.get('embedding_dim', 300)\n",
        "        self.target_dim = param_dict.get('target_dim', 2)\n",
        "        \n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, model_param: ModelParam):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            model_param.vocab_size, \n",
        "            model_param.embedding_dim\n",
        "        )\n",
        "        self.lin = nn.LSTM(\n",
        "            model_param.input_size * model_param.embedding_dim, \n",
        "            model_param.target_dim\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.embedding(x).view(x.size()[0], -1)\n",
        "        features = F.relu(features)\n",
        "        features = self.lin(features)\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ziseZlBnEQc"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #print(src.shape)\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        #print(embedded.shape)\n",
        "        \n",
        "        #embedded = embedded.reshape(1, embedded.shape[0], embedded.shape[1])\n",
        "        \n",
        "        #print(embedded.unsqueeze(-1).shape)\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNXPr_JYnLqU"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        #print(prediction)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKehKGXdnQUu"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw38HjTsgY7M"
      },
      "source": [
        "INPUT_DIM = len(inp_fields.vocab)\n",
        "OUTPUT_DIM = len(out_fields.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 5\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "#print(dec)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WgIpBM2n-vV",
        "outputId": "d974765c-765f-4f80-b610-3dc815ec07ad"
      },
      "source": [
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3366, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=5, dropout=0.3)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1245, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=5, dropout=0.3)\n",
              "    (fc_out): Linear(in_features=512, out_features=1245, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKlbxaJGoD0D",
        "outputId": "6f958b0f-bbfd-496d-eddf-8f16410bb202"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 21,783,005 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjLu8a0moR5K"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRG_PAD_IDX = inp_fields.vocab.stoi[inp_fields.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uC6fc2noqj6"
      },
      "source": [
        "def train(epoch, model, iterator, optimizer, criterion, clip):\n",
        "    #epoch, model, optimizer = load_checkpoint(PATH, model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.text.to(device)\n",
        "        #print(src.shape)\n",
        "        \n",
        "        trg = batch.label.to(device)\n",
        "        #print(trg.shape)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        #print(output, trg)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    checkpoint = {\"state_dict\" : model.state_dict(), \"optimizer\" : optimizer.state_dict(), 'epoch' : epoch}\n",
        "    save_checkpoint(checkpoint)\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjfbigdjWopa"
      },
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNef816cAaf"
      },
      "source": [
        "bptt = 35\n",
        "from typing import Tuple\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fg_NNzEU_vo"
      },
      "source": [
        "def Persuasive_sentence(model, sentence, input, per_out, device, max_length=50):\n",
        "\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text for token in eng(sentence)]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, input.init_token)\n",
        "    tokens.append(input.eos_token)\n",
        "\n",
        "    # Go through each german token and convert to an index\n",
        "    text_to_indices = [input.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).to(device).reshape(1,-1).t()\n",
        "    print(sentence_tensor.shape) #->67\n",
        "    #sentence_tensor = batchify(sentence_tensor, 67)\n",
        "    #print(sentence_tensor.shape)\n",
        "    \n",
        "\n",
        "    outputs = [per_out.vocab.stoi[\"<sos>\"]]\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "        print(trg_tensor.shape)\n",
        "        #data, targ = get_batch(sentence_tensor, 0)\n",
        "        #print(data.shape, targ.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "            #output = model(data, targ)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == per_out.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [per_out.vocab.itos[idx] for idx in outputs]\n",
        "    # remove start token\n",
        "    return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF4Qf4KBow8B"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.text.to(device)\n",
        "            trg = batch.label.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "            #print(output)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqrA4Q9Oo1ob"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpyzPwtv0v3w"
      },
      "source": [
        "path = %pwd\n",
        "PATH = os.path.join(path, 'my_checkpointseq.pth.tar')\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpointseq.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(PATH, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    epoch = checkpoint['epoch']\n",
        "    return epoch, model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zggmqVaho_JU",
        "outputId": "00b08275-e3f8-42f0-a4ee-d8e2d2fc4d0d"
      },
      "source": [
        "import time\n",
        "import math \n",
        "\n",
        "N_EPOCHS = 15\n",
        "CLIP = 1\n",
        "\n",
        "test_loss_l = []\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "sepoch = 0\n",
        "\n",
        "#sepoch, model, optimizer = load_checkpoint(PATH, model, optimizer)\n",
        "\n",
        "for epoch in range(sepoch, N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(epoch, model, train_itr, optimizer, criterion, CLIP)\n",
        "    writer.add_scalar(\"Training loss\", train_loss, global_step=epoch)\n",
        "    valid_loss = evaluate(model, test_itr, criterion)\n",
        "    writer.add_scalar(\"Validation loss\", valid_loss, global_step=epoch)\n",
        "    test_loss_l.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "Epoch: 01 | Time: 5m 41s\n",
            "\tTrain Loss: 4.619 | Train PPL: 101.376\n",
            "\t Val. Loss: 4.481 |  Val. PPL:  88.351\n",
            "=> Saving checkpoint\n",
            "Epoch: 02 | Time: 5m 39s\n",
            "\tTrain Loss: 4.225 | Train PPL:  68.374\n",
            "\t Val. Loss: 4.539 |  Val. PPL:  93.554\n",
            "=> Saving checkpoint\n",
            "Epoch: 03 | Time: 5m 39s\n",
            "\tTrain Loss: 3.789 | Train PPL:  44.193\n",
            "\t Val. Loss: 5.421 |  Val. PPL: 226.101\n",
            "=> Saving checkpoint\n",
            "Epoch: 04 | Time: 5m 39s\n",
            "\tTrain Loss: 3.045 | Train PPL:  21.018\n",
            "\t Val. Loss: 6.462 |  Val. PPL: 640.267\n",
            "=> Saving checkpoint\n",
            "Epoch: 05 | Time: 5m 39s\n",
            "\tTrain Loss: 2.475 | Train PPL:  11.886\n",
            "\t Val. Loss: 7.415 |  Val. PPL: 1661.349\n",
            "=> Saving checkpoint\n",
            "Epoch: 06 | Time: 5m 38s\n",
            "\tTrain Loss: 2.140 | Train PPL:   8.503\n",
            "\t Val. Loss: 6.971 |  Val. PPL: 1065.267\n",
            "=> Saving checkpoint\n",
            "Epoch: 07 | Time: 5m 38s\n",
            "\tTrain Loss: 1.925 | Train PPL:   6.855\n",
            "\t Val. Loss: 8.144 |  Val. PPL: 3441.632\n",
            "=> Saving checkpoint\n",
            "Epoch: 08 | Time: 5m 38s\n",
            "\tTrain Loss: 1.785 | Train PPL:   5.962\n",
            "\t Val. Loss: 8.439 |  Val. PPL: 4625.737\n",
            "=> Saving checkpoint\n",
            "Epoch: 09 | Time: 5m 38s\n",
            "\tTrain Loss: 1.689 | Train PPL:   5.412\n",
            "\t Val. Loss: 8.220 |  Val. PPL: 3712.879\n",
            "=> Saving checkpoint\n",
            "Epoch: 10 | Time: 5m 39s\n",
            "\tTrain Loss: 1.572 | Train PPL:   4.815\n",
            "\t Val. Loss: 8.308 |  Val. PPL: 4054.357\n",
            "=> Saving checkpoint\n",
            "Epoch: 11 | Time: 5m 39s\n",
            "\tTrain Loss: 1.507 | Train PPL:   4.513\n",
            "\t Val. Loss: 7.981 |  Val. PPL: 2923.530\n",
            "=> Saving checkpoint\n",
            "Epoch: 12 | Time: 5m 39s\n",
            "\tTrain Loss: 1.448 | Train PPL:   4.257\n",
            "\t Val. Loss: 7.894 |  Val. PPL: 2680.378\n",
            "=> Saving checkpoint\n",
            "Epoch: 13 | Time: 5m 39s\n",
            "\tTrain Loss: 1.383 | Train PPL:   3.987\n",
            "\t Val. Loss: 8.409 |  Val. PPL: 4486.853\n",
            "=> Saving checkpoint\n",
            "Epoch: 14 | Time: 5m 39s\n",
            "\tTrain Loss: 1.352 | Train PPL:   3.864\n",
            "\t Val. Loss: 8.594 |  Val. PPL: 5397.737\n",
            "=> Saving checkpoint\n",
            "Epoch: 15 | Time: 5m 39s\n",
            "\tTrain Loss: 1.301 | Train PPL:   3.672\n",
            "\t Val. Loss: 9.082 |  Val. PPL: 8794.049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2gQJGN3cRtXS",
        "outputId": "f14416cb-3fd1-4768-f862-da5bc47c7eae"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_itr, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 4.481 | Test PPL:  88.351 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aDXy8EDCS8Rw",
        "outputId": "6a09a8b1-71b1-490b-97cb-d611fba15ee2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(N_EPOCHS), test_loss_l)\n",
        "plt.xlabel('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, '')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bhVKj-HcEeE5"
      },
      "source": [
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"text\"]\n",
        "        trg = vars(example)[\"label\"]\n",
        "\n",
        "        prediction = Persuasive_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4DQQ2o3_GeRJ",
        "outputId": "3f2c889b-63bf-42dd-a517-6fe937705170"
      },
      "source": [
        "model.eval()\n",
        "print(inputs[3])\n",
        "print(Persuasive_sentence(model, inputs[3], inp_fields, out_fields, device, max_length=50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dusty Pink Embroidered Semi - Stitched Lehenga & amp ; Unstitched Blouse With Dupatta SHUBHKALA Dusty pink embroidered lehenga choli with dupatta Dusty pink embroidered unstitched blouse with beaded detail Dusty pink embroidered semi - stitched lehenga with beaded detail , has drawstring and concealed zip closure , flared hem , and an attached lining Dusty pink embroidered dupatta Material : Viscose Rayon Dry Clean\n",
            "torch.Size([67, 1])\n",
            "torch.Size([1, 1])\n",
            "torch.Size([2, 1])\n",
            "torch.Size([3, 1])\n",
            "torch.Size([4, 1])\n",
            "torch.Size([5, 1])\n",
            "torch.Size([6, 1])\n",
            "torch.Size([7, 1])\n",
            "torch.Size([8, 1])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([10, 1])\n",
            "torch.Size([11, 1])\n",
            "torch.Size([12, 1])\n",
            "torch.Size([13, 1])\n",
            "torch.Size([14, 1])\n",
            "torch.Size([15, 1])\n",
            "torch.Size([16, 1])\n",
            "torch.Size([17, 1])\n",
            "torch.Size([18, 1])\n",
            "torch.Size([19, 1])\n",
            "torch.Size([20, 1])\n",
            "torch.Size([21, 1])\n",
            "torch.Size([22, 1])\n",
            "torch.Size([23, 1])\n",
            "torch.Size([24, 1])\n",
            "torch.Size([25, 1])\n",
            "torch.Size([26, 1])\n",
            "torch.Size([27, 1])\n",
            "torch.Size([28, 1])\n",
            "torch.Size([29, 1])\n",
            "torch.Size([30, 1])\n",
            "torch.Size([31, 1])\n",
            "torch.Size([32, 1])\n",
            "torch.Size([33, 1])\n",
            "['<unk>', 'This', 'your', 'and', 'with', 'this', 'this', 'with', 'this', 'kurta', 'kurta', 'from', 'from', '.', '.', '.', 'your', 'your', 'with', 'with', 'with', 'this', 'this', 'this', 'and', 'a', 'with', 'with', 'with', 'and', 'and', 'and', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_kTMh0AaMQI"
      },
      "source": [
        "score = bleu(test_data[1:100], model, inp_fields, out_fields, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G_NqHAacvBN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}